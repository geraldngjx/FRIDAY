{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_5jDYNbrzSN"
      },
      "source": [
        "### Exploratory Data Analysis \n",
        "This notebook contains code that explores the [Amazon Reviews Dataset](https://jmcauley.ucsd.edu/data/amazon_v2/index.html).\n",
        "\n",
        "\n",
        "Pinecone: Descriptions + reviews \n",
        "\n",
        "Finetuning: Question and Answer (context is reviews and descriptions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PcpubL-z5Bh",
        "outputId": "a070ee88-dd1f-4978-c590-9a1eadd35761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.8/dist-packages (2.2.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.11.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.25.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.0+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FUzYdAjFrzSP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import gzip\n",
        "import json\n",
        "import requests\n",
        "from io import BytesIO, StringIO\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_ = urllib.request.urlretrieve(\"https://amazon-qa.s3-us-west-2.amazonaws.com/train-qar.jsonl\", \"train-qar.jsonl\")\n",
        "_ = urllib.request.urlretrieve(\"https://amazon-qa.s3-us-west-2.amazonaws.com/val-qar.jsonl\", \"var-qar.jsonl\")"
      ],
      "metadata": {
        "id": "Vhlxb3OsnN9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(url,N,downloaded):\n",
        "  im_path = url.split('/')[-1]\n",
        "  final_path = im_path.replace('.gz','')\n",
        "  if not downloaded:\n",
        "    _ = urllib.request.urlretrieve(url, im_path)\n",
        "    with gzip.open(im_path, 'rb') as infile:\n",
        "      with open(final_path, 'wb') as outfile:\n",
        "          for line in infile:\n",
        "              outfile.write(line)\n",
        "  it = pd.read_json(final_path,chunksize = 1000,lines= True)\n",
        "  first_n_rows = pd.DataFrame()\n",
        "  for chunk in it:\n",
        "      first_n_rows = first_n_rows.append(chunk.head(N))\n",
        "      if len(first_n_rows) >= N:\n",
        "          break\n",
        "  return first_n_rows"
      ],
      "metadata": {
        "id": "2Oc9RGgRl-6I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desc = get_data('https://jmcauley.ucsd.edu/data/amazon_v2/metaFiles2/meta_Toys_and_Games.json.gz',500000,True) #first 500000 rows"
      ],
      "metadata": {
        "id": "xo181g2CnFRC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CSblCvEurzSQ"
      },
      "outputs": [],
      "source": [
        "desc['description'] = desc['description'].apply(lambda item: ' '.join([y for y in ''.join(item).split('<') if '>' not in y]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4EoOXGZWrzSQ"
      },
      "outputs": [],
      "source": [
        "df_desc = desc[['title','asin', 'description']].drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A_MM0MUlrzSQ"
      },
      "outputs": [],
      "source": [
        "df_ids = desc['asin'].unique().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_jsonl(path, df_ids):\n",
        "  N = 2000\n",
        "  counter = 0\n",
        "  dfs = []\n",
        "  with open(path) as f:\n",
        "    for i, line in enumerate(f):\n",
        "      try:\n",
        "        if json.loads(line).get('asin') in df_ids:\n",
        "          dfs.append(pd.json_normalize(json.loads(line)))\n",
        "          counter += 1\n",
        "        if counter > N:\n",
        "          break\n",
        "      except:\n",
        "        pass\n",
        "  return dfs\n"
      ],
      "metadata": {
        "id": "aQENYnHqgQ74"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = parse_jsonl('train-qar.jsonl',df_ids)\n",
        "var = parse_jsonl('var-qar.jsonl',df_ids)\n",
        "test = parse_jsonl('test-qar_all.jsonl',df_ids)"
      ],
      "metadata": {
        "id": "iJr9wftYsfM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master = pd.concat(train + var + test).merge(df_desc, how = 'left', on = 'asin')"
      ],
      "metadata": {
        "id": "yOxHLpCi_9em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = master[master['is_answerable'] == 1].explode('answers')\n",
        "df['answers'] = df['answers'].apply(lambda x: x.get('answerText'))"
      ],
      "metadata": {
        "id": "OH8o9nM6rMkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_data = master[['review_snippets','questionText','is_answerable']].copy()\n",
        "model_data['review_snippets'] = model_data['review_snippets'].apply(lambda x: '    '.join(x)) #4 spaces"
      ],
      "metadata": {
        "id": "Tgw_kthkv7AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_data['is_answerable'].value_counts()"
      ],
      "metadata": {
        "id": "y52k8L4uyB4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = model_data.drop('is_answerable',axis=1)\n",
        "y = model_data['is_answerable'].values"
      ],
      "metadata": {
        "id": "FuIDT7rIyPb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42,shuffle = True,stratify = y)"
      ],
      "metadata": {
        "id": "u2GW8hSFv67-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer\n",
        "from nltk import tokenize\n",
        "import pickle"
      ],
      "metadata": {
        "id": "fRNfmY7Ov65j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words ='english')\n",
        "sentence_t = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')"
      ],
      "metadata": {
        "id": "UuQk28l7CK5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer.fit(X_train['questionText'].tolist() + X_train['review_snippets'].tolist())"
      ],
      "metadata": {
        "id": "qTD0ASi5HEXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_features(df):\n",
        "  #HELPERS\n",
        "  def intersec(q,r):\n",
        "    return len([x for x in q if x in r])\n",
        "  \n",
        "  def vectorize(text):\n",
        "    return tfidf_vectorizer.transform([text]).toarray()[0]\n",
        "\n",
        "  def tokenize(text):\n",
        "    return tokenizer(text,max_length=512,truncation=True).get('input_ids')\n",
        "  \n",
        "  def sentenceTransform(text):\n",
        "    return sentence_t.encode(text)\n",
        "\n",
        "  def cosine_similarity(a,b):\n",
        "    return np.dot(a,b)/(np.linalg.norm(a) * np.linalg.norm(b))\n",
        "  \n",
        "  def dotproduct(a,b):\n",
        "    return np.dot(a,b)\n",
        "  \n",
        "  def euclid_dist(a,b):\n",
        "    return np.linalg.norm(a-b)\n",
        "  \n",
        "  def sentence_mean_max(model,q,r):\n",
        "    f = dotproduct\n",
        "    vals = [f(q,model(sentence)) for sentence in tokenize.sent_token]\n",
        "    return pd.Series([np.mean(vals), np.max(vals)])\n",
        "\n",
        "  df = df.copy()\n",
        "  df['question_ntokens'] = df['questionText'].apply(lambda x:len(tokenize(x)))\n",
        "  df['review_ntokens'] = df['review_snippets'].apply(lambda x:len(tokenize(x)))\n",
        "  df['question_tokens'] = df['questionText'].apply(lambda x:tokenize(x))\n",
        "  df['review_tokens'] = df['review_snippets'].apply(lambda x:tokenize(x))\n",
        "  df['intersec'] = df.apply(lambda row: intersec(row['questionText'], row['review_snippets']),axis=1)\n",
        "  df['intersec_pct'] = df['intersec'] / df['question_ntokens']\n",
        "  df['question_encoded'] = df['questionText'].apply(lambda x: sentenceTransform(x))\n",
        "  df['review_encoded'] = df['review_snippets'].apply(lambda x: sentenceTransform(x))\n",
        "  df['question_tfidf'] = df['questionText'].apply(lambda x: vectorize(x))\n",
        "  df['review_tfidf'] = df['review_snippets'].apply(lambda x: vectorize(x))\n",
        "\n",
        "  for m in ['encoded','tfidf']:\n",
        "    df[f'cosine_sim_{m}'] = df.apply(lambda row: cosine_similarity(row[f'question_{m}'], row[f'review_{m}']),axis =1 )\n",
        "    df[f'dot_prod_{m}'] = df.apply(lambda row: dotproduct(row[f'question_{m}'], row[f'review_{m}']),axis =1 )\n",
        "    df[f'euclid_dist_{m}'] = df.apply(lambda row: euclid_dist(row[f'question_{m}'], row[f'review_{m}']),axis =1 )\n",
        "  df[['sent_max_encoded','sent_mean_encoded']] = df.apply(lambda row: sentence_mean_max(sentenceTransform, row['question_encoded'], row['review_snippets']),axis =1 )\n",
        "  df[['sent_max_tfidf','sent_mean_tfidf']] = df.apply(lambda row: sentence_mean_max(vectorize, row['question_tfidf'], row['review_snippets']),axis =1 )\n",
        "\n",
        "  df.drop(['question_tokens','review_tokens','question_encoded','review_encoded','question_tfidf','review_tfidf'],axis =1 , inplace = True)\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "QA2RJ3qL7rSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = add_features(X_train)"
      ],
      "metadata": {
        "id": "3Mu_MaW2KVvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.drop(['review_snippets','questionText'],axis =1 ,inplace = True)"
      ],
      "metadata": {
        "id": "_9Y2vwo8TBiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = add_features(X_test)\n",
        "X_test.drop(['review_snippets','questionText'],axis =1 ,inplace = True)"
      ],
      "metadata": {
        "id": "AfKd38zWTZe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import log_loss, roc_auc_score, precision_score"
      ],
      "metadata": {
        "id": "p-cYR2igQ3qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight = (len(y) - y.sum())/y.sum()\n",
        "clf = XGBClassifier(random_state = 42,n_jobs = - 1,eval_metric = 'logloss',scale_pos_weight = weight,booster = 'dart')\n",
        "clf = clf.fit(X_train, y_train,eval_set=[(X_test,y_test)],verbose=1000)"
      ],
      "metadata": {
        "id": "3uizEo5AQ9Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = clf.predict(X_test)\n",
        "print('precision:', precision_score(preds,y_test))\n",
        "print('roc-auc:', roc_auc_score(preds,y_test))\n",
        "print('loss:', log_loss(preds,y_test))"
      ],
      "metadata": {
        "id": "QhagTpWyQ88A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('classifer.pkl', 'wb') as f:\n",
        "    pickle.dump(clf, f)\n",
        "from google.colab import files\n",
        "files.download('classifer.pkl') "
      ],
      "metadata": {
        "id": "MHTlXqWDU0PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# master['pct'] = master.apply(lambda row: len(list(set(row['description'].split(' ')) & set(row['answers'].split(' ')))) / len(set(row['answers'].split(' '))), axis = 1)\n",
        "# master[master['pct'] > 0.2] #Get all records where the at least 20% of the answer is inside the product description"
      ],
      "metadata": {
        "id": "Yf5R9UFUd7fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('data.csv') "
      ],
      "metadata": {
        "id": "1AGpwExlASMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zc3YDSKbagUT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sports",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ec142a6a42d37bb51ddd41d81bbd8744c33749e62da99fedbc872fac0e9f4b71"
      }
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}